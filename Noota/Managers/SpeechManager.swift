import Foundation
import Speech
import Combine
import SwiftUI

class SpeechManager: ObservableObject {
    @Published var isRecording = false
    @Published var recognizedText = ""
    @Published var liveRecognizedText = ""
    @Published var error: Error?
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    private var audioRecorder: AVAudioRecorder?
    private var recordingURL: URL?
    
    //  Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø±
    private var isContinuousMode = false
    private var currentLanguageCode = "en-US"
    
    //  Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ - ØªØªØ¨Ø¹ Ø¢Ø®Ø± Ù†Øµ Ø£Ø±Ø³Ù„Ù†Ø§Ù‡
    private var sentenceBuffer = ""
    private var lastSentIndex = 0 // Ø¢Ø®Ø± Ù…ÙˆØ¶Ø¹ Ø£Ø±Ø³Ù„Ù†Ø§Ù‡
    private var processingTimer: Timer?
    private let sentenceCompletionDelay: TimeInterval = 1.0
    
    //  Subject Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©
    private let completedSentenceSubject = PassthroughSubject<String, Never>()
    var completedSentencePublisher: AnyPublisher<String, Never> {
        completedSentenceSubject.eraseToAnyPublisher()
    }
    
    init() {
        requestAuthorization()
    }
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    Logger.log("Speech recognition authorization granted.", level: .info)
                case .denied, .restricted, .notDetermined:
                    self.error = NSError(domain: "SpeechManagerError", code: 1, userInfo: [NSLocalizedDescriptionKey: "Speech recognition authorization denied."])
                    Logger.log("Speech recognition authorization failed.", level: .error)
                @unknown default:
                    self.error = NSError(domain: "SpeechManagerError", code: 2, userInfo: [NSLocalizedDescriptionKey: "Unknown authorization status."])
                    Logger.log("Unknown speech recognition authorization status.", level: .error)
                }
            }
        }
    }
    
    //  Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø± (Ø¶ØºØ·Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)
    func startContinuousRecording(languageCode: String) {
        //  Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù†Ø´Ø·Ø§Ù‹ØŒ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø·Ù„Ø¨
        guard !isContinuousMode else {
            Logger.log("â¸ï¸ Continuous recording already active. Ignoring request.", level: .info)
            return
        }
        
        guard SFSpeechRecognizer.authorizationStatus() == .authorized else {
            requestAuthorization()
            return
        }
        
        do {
            isContinuousMode = true
            currentLanguageCode = languageCode
            resetState()
            
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            
            startRecognitionSession()
            
            Logger.log(" Continuous speech recording started for language: \(languageCode).", level: .info)
            
        } catch {
            self.error = error
            isContinuousMode = false
            Logger.log("Failed to start continuous recording: \(error.localizedDescription)", level: .error)
        }
    }
    
    //  Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…
    private func startRecognitionSession() {
        guard isContinuousMode else { return }
        
        Logger.log("Starting new recognition session...", level: .debug)
        
        //  Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
        stopCurrentRecognitionSession()
        
        do {
            isRecording = true
            
            let recognizer = SFSpeechRecognizer(locale: Locale(identifier: currentLanguageCode))
            
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            recognitionRequest?.shouldReportPartialResults = true
            
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, when in
                self?.recognitionRequest?.append(buffer)
            }
            
            if !audioEngine.isRunning {
                audioEngine.prepare()
                try audioEngine.start()
            }
            
            recognitionTask = recognizer?.recognitionTask(with: recognitionRequest!) { [weak self] result, error in
                guard let self = self else { return }
                
                DispatchQueue.main.async {
                    self.handleContinuousRecognitionResult(result: result, error: error)
                }
            }
            
            Logger.log(" Recognition session started successfully", level: .debug)
            
        } catch {
            self.error = error
            Logger.log("Failed to start recognition session: \(error.localizedDescription)", level: .error)
            
            //  Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ Ø«Ø§Ù†ÙŠØªÙŠÙ†
            DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
                if self.isContinuousMode {
                    self.stopCurrentRecognitionSession()
                    self.startRecognitionSession()
                }
            }
        }
    }
    
    //  Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„Ù…Ø³ØªÙ…Ø±
    private func handleContinuousRecognitionResult(result: SFSpeechRecognitionResult?, error: Error?) {
        if let result = result {
            let newText = result.bestTranscription.formattedString
            self.liveRecognizedText = newText
            
            //  Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¬Ø¯ÙŠØ¯
            if !newText.isEmpty {
                processPendingText(newText)
            }
            
            //  Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù†Ù‡Ø§Ø¦ÙŠØ©ØŒ Ø£Ø¹Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø© ÙÙˆØ±Ø§Ù‹
            if result.isFinal {
                Logger.log(" Result is final, restarting session for next sentence...", level: .debug)
                if isContinuousMode {
                    //  Ø§Ù…Ø³Ø­ Ø§Ù„Ø¨Ø§ÙØ± Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø¬Ø¯ÙŠØ¯
                    sentenceBuffer = ""
                    liveRecognizedText = ""
                    
                    //  Ø¨Ø¯ÙˆÙ† ØªØ£Ø®ÙŠØ± - restart ÙÙˆØ±ÙŠ
                    stopCurrentRecognitionSession()
                    DispatchQueue.main.async { [weak self] in
                        guard let self = self, self.isContinuousMode else { return }
                        self.startRecognitionSession()
                    }
                }
            }
        }
        
        //  ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ØŒ Ø£Ø¹Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø©
        if let recognitionError = error {
            Logger.log(" Recognition error: \(recognitionError.localizedDescription)", level: .warning)
            if isContinuousMode {
                //  Ø§Ù…Ø³Ø­ Ø§Ù„Ø¨Ø§ÙØ± Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø·Ø£ Ø£ÙŠØ¶Ø§Ù‹
                sentenceBuffer = ""
                liveRecognizedText = ""
                
                //  ØªØ£Ø®ÙŠØ± ØµØºÙŠØ± Ù‚Ø¨Ù„ restart ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
                    self.stopCurrentRecognitionSession()
                    self.startRecognitionSession()
                }
            }
        }
    }
    
    //  Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ù„Ù‚ ÙˆÙƒØ´Ù Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©
    private func processPendingText(_ newText: String) {
        let cleanedText = newText.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !cleanedText.isEmpty else { return }
        
        sentenceBuffer = cleanedText
        
        //  ÙƒØ´Ù Ù†Ù‚Ø·Ø© Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… Ø£Ùˆ Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©
        if shouldSendNow(cleanedText) {
            sendCompletedSentence()
            return
        }
        
        // Ø¥Ø°Ø§ Ù„Ù… Ù†Ø±Ø³Ù„ØŒ Ø§Ù†ØªØ¸Ø± Ø´ÙˆÙŠØ©
        resetProcessingTimer()
    }
    
    //  ØªØ­Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø¢Ù†
    private func shouldSendNow(_ text: String) -> Bool {
        let finalPunctuation: Set<Character> = [".", "!", "?", "ØŸ"]
        
        // Ø¹Ù„Ø§Ù…Ø© ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ø¶Ø­Ø© = Ø¥Ø±Ø³Ù„
        if let lastChar = text.last, finalPunctuation.contains(lastChar) {
            return true
        }
        
        // Ø¬Ù…Ù„Ø© Ø·ÙˆÙŠÙ„Ø© (15+ ÙƒÙ„Ù…Ø©) = Ø§Ø­ØªÙ…Ù„ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ÙÙƒØ±Ø©
        let wordCount = text.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty }.count
        if wordCount >= 15 {
            return true
        }
        
        return false
    }
    
    
    //  Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø© - Ø§Ø­ØªØ±Ø§ÙÙŠ ÙˆØ³Ù„Ø³
    private func sendCompletedSentence() {
        let cleanSentence = sentenceBuffer.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !cleanSentence.isEmpty else { return }
        
        recognizedText = cleanSentence
        
        Logger.log("ğŸ“¤ Sending sentence: '\(cleanSentence)'", level: .info)
        
        // Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· Ø§Ù„Ø¨Ø§ÙØ± Ù„Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
        sentenceBuffer = ""
        liveRecognizedText = ""
        processingTimer?.invalidate()
        processingTimer = nil
        
        // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¹Ø¨Ø± Publisher
        completedSentenceSubject.send(cleanSentence)
    }
    
    //  Ù…Ø¤Ù‚Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©
    private func resetProcessingTimer() {
        processingTimer?.invalidate()
        processingTimer = Timer.scheduledTimer(withTimeInterval: sentenceCompletionDelay, repeats: false) { [weak self] _ in
            self?.processBufferedSentence()
        }
    }
    
    //  Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ø§Ù„Ø¨Ø§ÙØ± Ø¨Ø¹Ø¯ Ø§Ù„ØµÙ…Øª
    private func processBufferedSentence() {
        guard !sentenceBuffer.isEmpty else { return }
        
        // Ø¥Ø°Ø§ ÙƒØ§Ù† ÙÙŠÙ‡ Ù†Øµ ÙÙŠ Ø§Ù„Ø¨Ø§ÙØ± = Ø£Ø±Ø³Ù„Ù‡
        sendCompletedSentence()
    }

    
    //  Ø¥ÙŠÙ‚Ø§Ù Ø¬Ù„Ø³Ø© Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„Ø­Ø§Ù„ÙŠØ©
    private func stopCurrentRecognitionSession() {
        Logger.log("Stopping current recognition session...", level: .debug)
        
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        if audioEngine.isRunning {
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        isRecording = false
        Logger.log(" Recognition session stopped", level: .debug)
    }
    
    //  Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹
    func stopContinuousRecording() {
        guard isContinuousMode else { return }
        
        isContinuousMode = false
        processingTimer?.invalidate()
        processingTimer = nil
        
        stopCurrentRecognitionSession()
        
        if audioEngine.isRunning {
            audioEngine.stop()
        }
        
        Logger.log("Continuous speech recording stopped.", level: .info)
    }
    
    //  Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· Ø§Ù„Ø­Ø§Ù„Ø©
    private func resetState() {
        liveRecognizedText = ""
        recognizedText = ""
        sentenceBuffer = ""
        processingTimer?.invalidate()
        processingTimer = nil
    }
    
    //  Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù„ØªÙˆØ§ÙÙ‚ (Ù„ÙƒÙ† ØªØ¹ÙŠØ¯ ØªÙˆØ¬ÙŠÙ‡ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯)
    func startRecording(languageCode: String) {
        startContinuousRecording(languageCode: languageCode)
    }
    
    func stopRecording() {
        //  ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±ØŒ Ù‡Ø°Ø§ Ù„Ø§ ÙŠÙˆÙ‚Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¨Ù„ ÙŠØ±Ø³Ù„ Ù…Ø§ ÙÙŠ Ø§Ù„Ø¨Ø§ÙØ±
        if isContinuousMode && !sentenceBuffer.isEmpty {
            processBufferedSentence()
        }
    }
    
    ///  Ù…Ø³Ø­ Ø§Ù„Ù€ buffer - Ø¨Ø³ÙŠØ· ÙˆÙØ¹Ù‘Ø§Ù„ Ø¬Ø¯Ø§Ù‹
    func clearRecognitionBuffer() {
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            
            Logger.log("ğŸ§¹ Clearing recognition buffer...", level: .debug)
            
            //  Ù…Ø³Ø­ Ø§Ù„Ø¨Ø§ÙØ± ÙÙ‚Ø·
            self.liveRecognizedText = ""
            self.recognizedText = ""
            self.sentenceBuffer = ""
            
            //  Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¤Ù‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©
            self.processingTimer?.invalidate()
            self.processingTimer = nil
            
            Logger.log(" Buffer cleared and ready for next sentence", level: .info)
        }
    }
    
    func reset() {
        stopContinuousRecording()
        resetState()
        error = nil
        Logger.log("SpeechManager state reset.", level: .info)
    }
    
    deinit {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
        processingTimer?.invalidate()
        audioRecorder?.stop()
        Logger.log("SpeechManager deinitialized.", level: .info)
    }
    
    // MARK: - Audio Recording
    
    /// Ø´Ø±ÙˆØ¹ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª
    func startAudioRecording() {
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let fileName = "voice_\(UUID().uuidString).wav"
        recordingURL = documentsPath.appendingPathComponent(fileName)
        
        guard let recordingURL = recordingURL else {
            Logger.log("Failed to create recording URL", level: .error)
            return
        }
        
        let settings = [
            AVFormatIDKey: Int(kAudioFormatLinearPCM),
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 1,
            AVLinearPCMBitDepthKey: 16,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsFloatKey: false
        ] as [String: Any]
        
        do {
            audioRecorder = try AVAudioRecorder(url: recordingURL, settings: settings)
            audioRecorder?.record()
            Logger.log(" Audio recording started: \(fileName)", level: .info)
        } catch {
            Logger.log("Failed to start audio recording: \(error.localizedDescription)", level: .error)
        }
    }
    
    /// ØªÙˆÙ‚Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
    func stopAudioRecording() -> URL? {
        audioRecorder?.stop()
        let url = recordingURL
        recordingURL = nil
        
        if let url = url {
            Logger.log(" Audio recording stopped: \(url.lastPathComponent)", level: .info)
        }
        
        return url
    }
}
